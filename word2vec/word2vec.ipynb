{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec"
      ],
      "metadata": {
        "id": "DTL89bL7J6CH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2vec** is a probabilistic method created by [Mikolov et al., 2013]. It is a software package that actually includes:\n",
        "\n",
        "$\\quad$ - **2 algorithmms**: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the *distribution*  (probability) of context words from a center word. In this Jupyter notebook, the second algorithm is implemented.\n",
        "\n",
        "$\\quad$ - **2 training methods**: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling *negative* examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary. In this Jupyter notebook, the negative sampling is implemented."
      ],
      "metadata": {
        "id": "YWmG957uRH3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-Gram model"
      ],
      "metadata": {
        "id": "loYkrNeQS_XR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notation for Skip-Gram Model:\n",
        "\n",
        "*   $w_i$: Word $i$ from vocabulary $V$\n",
        "*   $\\mathcal{V} \\in \\mathbb{R^{n \\times |V|}}$: Input word matrix\n",
        "*   $v_i$: $i$-th column of $\\mathcal{V}$, the input vector representation of word $w_i$\n",
        "*   $\\mathcal{U} \\in \\mathbb{R^{n \\times |V|}}$: Output word matrix\n",
        "*   $u_i$: $i$-th row of $\\mathcal{U}$, the output vector representation of word $w_i$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1PtGo_oTCq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's breakdown the way this model works in these 6 steps:\n",
        "1.   We generate our one hot input vector $x \\in \\mathbb{R^{|V|}}$ of the center word\n",
        "2.   We get our embedded word vector for the center word $v_c=\\mathcal{V}x \\in \\mathbb{R^{|V|}}$\n",
        "3.   Generate a score vector $z = \\mathcal{U}v_c$.\n",
        "4.   Turn the score vector into probabilities, $\\hat{y} = softmax(z)$. Note that $\\hat{y}_{c-m},...,\\hat{y}_{c-1}, \\hat{y}_{c+1},...,\\hat{y}_{c+m}$ are the probabilities of observing each context word.\n",
        "5.   We desire our probability vector generated to match the true probabiliies which is $y^{c-m},...,y^{c-1}, y^{c+1},...,y^{c+m}$, the one hot vectors of the actual output.\n",
        "\n"
      ],
      "metadata": {
        "id": "1oI2hshIVG8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective function to minimize:\n",
        "\n",
        "\\begin{align}\n",
        "   minimizeJ = - \\sum_{j=0,j \\ne m}^{2m}u_{c-m+j}^Tv_c + 2mlog\\sum_{k=1}^{|V|}exp(u_k^Tv_c)\n",
        "\\end{align}\n",
        "\n",
        "Loss functions in both algorithms are expensive to compute because of the softmax normalization, where we sum over all $|V|$ scores.\n",
        "\n",
        "Negative Sampling:\n",
        "\n",
        "For every training step, instead of looping over the entire vocabulary, we can just sample several negative examples! We 'sample' from a noise distribution ($P_n(w)$) whose probabilities match the ordering of the frequency of the vocabulary. To augment our formulation of the problem to incorporate Negative Sampling, all we need to do is update:\n",
        "\n",
        "*   objective function\n",
        "*   gradients\n",
        "*   update rules\n",
        "\n",
        "Mikolov et al. present Negative Sampling in Distributed Representations of Words and Phrases and their Compositionality. While negative sampling is based on the Skip-Gram model, it is in fact optimizing a different objective. Consider a pair\n",
        "($w, c$) of word and context.\n",
        "\n",
        "Did this pair come from the training data? Let’s denote by $P(D = 1|w, c)$ the probability that ($w, c$) came from the corpus data. Correspondingly, $P(D = 0|w, c)$ will be the probability that ($w, c$) did not come from the corpus data. First, let’s model $P(D = 1|w, c)$ with the sigmoid function:\n",
        "\n",
        "\\begin{align}\n",
        "   P(D = 1|w, c, \\theta) = \\sigma(v_c^Tv_w)=\\frac{1}{1+exp{(-v_c^Tv_w)}}\n",
        "\\end{align}\n",
        "\n",
        "Now, we build a new objective function that tries to maximize the probability of a word and context being in the corpus data if it indeed is, and maximize the probability of a word and context not being in the corpus data if it indeed is not. We take a simple maximum likelihood approach of these two probabilities. The maximizing the likelihood is the same as minimizing the negative log likelihood, then we have:\n",
        "\n",
        "\\begin{align}\n",
        "   J = - \\sum_{(w,c) \\in D}log\\frac{1}{1+exp{(-u_w^Tv_c)}} - \\sum_{(w,c) \\in \\tilde{D}}log\\frac{1}{1+exp{(u_w^Tv_c)}}\n",
        "\\end{align}\n",
        "\n",
        "Note that $\\tilde{D}$ is a 'false' or 'nagative' corpus. Where we would have sentences like 'stock boil fish is toy'. Unnatural sentences that should\n",
        "get a low probability of ever occurring. We can generate $\\tilde{D}$ on the fly by randomly sampling this negative from the word bank.\n",
        "\n",
        "So, for Skip-Gram Model we have a new objective function for observing the context word $c-m+j$ given the center word $c$ would be:\n",
        "\n",
        "\\begin{align}\n",
        "   -log \\ \\sigma(u_{c-m+j}^Tv_c) - \\sum_{k=1}^Klog \\ \\sigma(-\\tilde{u}_k^Tv_c)\n",
        "\\end{align}\n",
        "\n",
        "In the above formulation, {$\\tilde{u}_k|k=1...K$} are sampled from $P_n(w)$. Let’s discuss what $P_n(w)$ should be. While there is much discussion of what makes the best approximation, what seems to work best is the Unigram Model raised to the power of $3/4$. Why $3/4$? Here’s an example that might help gain some intuition:\n",
        "\n",
        "*   is: $0.9^{3/4} = 0.92$\n",
        "*   Constitution: $0.09^{3/4} = 0.16$\n",
        "*   bombastic: $0.01^{3/4} = 0.032$\n",
        "\n",
        "\"Bombastic\" is now 3x more likely to be sampled while \"is\" only went up marginally. "
      ],
      "metadata": {
        "id": "KpxvTR9EXlAa"
      }
    }
  ]
}